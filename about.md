---
layout: page
title: About
description: >-
    Course policies and information.
---

# About
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Goals of the Course

Natural language processing (NLP) seeks to endow computers with the ability to intelligently process human language. NLP components are used in conversational agents and other systems that engage in dialogue with humans, automatic translation between human languages, automatic answering of questions using large text collections, the extraction of structured information from text, tools that help human authors, and many, many more. This course will teach you the <b>fundamental ideas</b> used in key NLP components. It is organized into several “greatest hits” topics, each with a more-or-less self-contained lecture and associated readings, problems, and implementation exercises.


## 447 vs. 517

The courses are similar in breadth and use the same lecture content.  The projects are quite different; 447’s project is a predefined implementation problem that gives teams freedom in developing a solution.  It is designed to encourage iterative improvement and an understanding of inherent tradeoffs in building an NLP system.  517’s project is more research-oriented; it asks teams to reproduce experiments in recently published NLP papers.  Teams have great flexibility in the choice of a paper to reproduce.

Additionally, there will be differences in the assignments.


## Lectures

You are encouraged to come to live lectures and participate in the discussions.  Videorecorded lectures from 2021 are provided as a supplementary resource and in case you can't make it to a lecture (e.g., [because you're not feeling well](../safety)).  Note that the ordering of the lectures may be slightly different year to year, especially later in the quarter.  Also, be warned that these lectures are "compressed" in the sense that they go about 30% faster than a live lecture with student participation.  Most students will prefer to watch them in small segments, pausing frequently to take notes, and perhaps watch some segments again.

1. [Classification and multinomial logistic regression](https://drive.google.com/file/d/1Luwa-sn4t2Hu6IA_-cUWXaDvMkpft9E4/view?usp=sharing); [with captions](https://drive.google.com/file/d/1iRFKwz8IInkjDFWB5rU7RO9tGtVna6wF/view?usp=sharing); [transcript](https://drive.google.com/file/d/1cxtCdPySB1PL72EQSWJOy2tpGkf0kYWK/view?usp=sharing); [slides](https://drive.google.com/file/d/1u3hyvV7bnh11yY6jCOnKOzWyWU8yPw6u/view?usp=sharing)
1. [Language modeling, especially with neural networks](https://drive.google.com/file/d/1cK43rSzH491oI9NIrLlDAeP8P2F7LXTJ/view?usp=sharing); [with captions](https://drive.google.com/file/d/17_YfmZPma6AwwjA5wuUSVzJjL6Nblcf1/view?usp=sharing); [transcript](https://drive.google.com/file/d/1hweCGRWzlIYqvN1uINPICtZp46KpOY1s/view?usp=sharing); [slides](https://drive.google.com/file/d/15xk-qyd3DFBLBYlTBDegfuZJKElJxuk4/view?usp=sharing) 
1. [Vector embeddings for documents and words](https://drive.google.com/file/d/1L65GHmZxrGanQyc8n6ncLJ91xjcHFVi7/view?usp=sharing); [with captions](https://drive.google.com/file/d/1M1-jH9a6QMBuNqQ5kEgGEW0eseWxV2JS/view?usp=sharing); [transcript](https://drive.google.com/file/d/1Y28Q1_yxTSFdft_MY5UNjbnK2-iC_ZoU/view?usp=sharing); [slides](https://drive.google.com/file/d/1ZOTh6VgchorZxpscuy9ovv-6NVgyyH-B/view?usp=sharing) 
1. [Morphology and weighted finite-state transducers](https://drive.google.com/file/d/1MDj3JUBecLOqCMApOWlxG0ZOxmZcQC20/view?usp=sharing); [with captions](https://drive.google.com/file/d/1zXXPwAFycgIRK-25TctN5IIvo7W2H-ii/view?usp=sharing); [transcript](https://drive.google.com/file/d/16DyBtGwSOUHVcSMN-hvCWsc0awCyX_n2/view?usp=sharing); [slides](https://drive.google.com/file/d/1ejcGyncrh5lSe_P7TRX8Slj_roZUWq2p/view?usp=sharing) 
1. [Sequence labeling and conditional random fields](https://drive.google.com/file/d/1NeLhUxWBBbUSeC5oyz0krxppzlG_OB5V/view?usp=sharing); [with captions](https://drive.google.com/file/d/1uyoeC80ynsVmXjEl2hFZZDWQWHXI8kjF/view?usp=sharing); [transcript](https://drive.google.com/file/d/1G3Ox7tIrjQN9LEV4VX2UL3-lp1VSMANI/view?usp=sharing); [slides](https://drive.google.com/file/d/1eH4OzFMStk1svUZM-8Iiyssb0kOsDrBb/view?usp=sharing) 
1. [Translation and sequence-to-sequence models](https://drive.google.com/file/d/18J0RTgezne5rfu5f9ryaA4Yu1V567q28/view?usp=sharing); [with captions](https://drive.google.com/file/d/1Sej4uNP5bjH0Cot73QKVu5ymHbRWwbN7/view?usp=sharing); [transcript](https://drive.google.com/file/d/1UR1RuQCQHVHn4CL5KabtlnVK7DLnt0WK/view?usp=sharing); [slides](https://drive.google.com/file/d/1BZ6IKDjn12TI8Vg-uf0PvSMZg_C1T9gm/view?usp=sharing) 
1. [Syntax, semantics, and linguistic structure prediction](https://drive.google.com/file/d/1gGXlnv2livCAhH6CK3H-5ij1ZsBNRsOM/view?usp=sharing); [with captions](https://drive.google.com/file/d/1dkGLEjvFupyzBzpb426vkUVC0eMcE6Tu/view?usp=sharing); [transcript](https://drive.google.com/file/d/1ybQeIScWKpOYjq-DC18HWevgn4oDEXwh/view?usp=sharing); [slides](https://drive.google.com/file/d/1KGu3oxTRoLcvKQqPcRhHBuntDCyj6cj4/view?usp=sharing) 


## Grades

You will be evaluated based on individually completed assignments (50%), a project completed in a team of three (40%), and quizzes (10%).  Course staff may grant extra credit (up to 5%) to students who actively and meaningfully engage on the course discussion board.  To give you a rough idea of how your final grade depends on your total score (calculated out of 100 points), the mapping from points to a final grade (for CSE 447 students in winter 2021 who earned between 66 and 100 points) was approximately 0.3677 + 0.0363 * #points, rounded to the UW scale.  Depending on the distribution of scores this year, the mapping might, of course, be very different.  Final grades for students registered from CSE 447 and CSE 517 are calculated separately based on their respective distributions.

### Assignments

There will be nine graded assignments, roughly one per week.  You are encouraged to complete all of them on time.  If you submit an assignment within seven days of the due date, the TAs will calculate the grade you would have received if it had been on time, but you will receive a zero.  Your total assignment grade will be calculated as a weighted sum of your nine assignments' grades.  Most assignments will have a weight of one; your best two assignments will be doubly weighted, and your worst two assignments will get a weight of zero.  (This means that your highest-graded two assignments will be worth 11.11% of your grade, the middle five will be worth 5.56% of your grade, and the worst-graded two will be worth 0% of your grade.)  Because we have built this slack into the grading system, and because you will still receive feedback on work that is slightly late, there will be no exceptions to our policy of zero credit for late work.  **Precise assignment deadlines are shown on the [course calendar](../calendar).**

- [Assignment 0](assets/docs/A0.pdf), which is ungraded, is a tool for you to assess your preparedness for this course.
- [Assignment 1](assets/docs/A1.pdf) is designed to advance your understanding of text classification, feature design and selction, the evaluation of classifiers, and the mathematics of some important classification models.
- [Assignment 2](assets/docs/A2.pdf) is designed to advance your understanding of frequency information in text data and some mathematical properties of language models.
- [Assignment 3](assets/docs/A3.pdf) is designed to advance your understanding of language models.
- [Assignment 4](assets/docs/A4.pdf) is designed to advance your understanding of word embeddings and NLP models that make use of them.
- [Assignment 5](assets/docs/A5.pdf) is designed to advance your understanding of ethical matters that arise in NLP.
- [Assignment 6](assets/docs/A6.pdf) is designed to advance your understanding of sequence models. 
- [Assignment 7](assets/docs/A7.pdf) is designed to give you more practical hands-on experience with sequence labeling.
- [Assignment 8](assets/docs/A8.pdf)  is designed to advance your understanding of structured problems in NLP that deal with segmentation, trees, and logical forms.
- [Assignment 9](assets/docs/A9.pdf) is designed to give you more practical hands-on experience with sequence-to-sequence methods.

Solving extra credit problems in an assignment increases your grade for that particular assignment.

### Project 

Details on the projects for CSE 447 and CSE 517 are given in the instructions documents below.  Project deliverables must be turned in on time; there will be zero credit for late submissions.  **Project deadlines are shown on the [course calendar](../calendar).**

- [CSE 447 project instructions](assets/docs/project-447.pdf)
- [CSE 517 project instructions](assets/docs/project-517.pdf) and [latex template](assets/templates/project-517.tgz)
- [Individual project updates](https://forms.gle/5sT36SqxhDKzgn259)


### Quizzes

There will be quizzes posted on Canvas, roughly once per week.  These are not graded on correctness; as long as you submit an attempt on time, you will earn full credit for the quiz.

## Computing

Please see information about computing resources [here](https://gist.github.com/jerome9189/b5686fdad24532901be7705f1616cd99).

## Resources

- Textbook:  [Introduction to Natural Language Processing by Jacob Eisenstein (2019).  MIT Press.](https://www.amazon.com/Introduction-Language-Processing-Adaptive-Computation/dp/0262042843/)


Enrichment lectures and more:

- Claire Cardie, [Information Extraction Through the Years:  How Did We Get Here?](https://slideslive.com/38938634/information-extraction-through-the-years-how-did-we-get-here)
- Yejin Choi, [Intuitive Reasoning as (Un)supervised Neural Generation](https://www.youtube.com/watch?v=h2wzQKRAdA8&ab_channel=MITEmbodiedIntelligence)
- Charles Isbell, [You Can’t Escape Hyperparameters and Latent Variables:  Machine Learning as a Software Engineering Enterprise](https://neurips.cc/virtual/2020/public/invited_16166.html)
- Kathy McKeown, [Rewriting the Past: Assessing the Field through the Lens of Language Generation](https://slideslive.com/38929460/rewriting-the-past-assessing-the-field-through-the-lens-of-language-generation)
- [NLP Highlights](https://soundcloud.com/nlp-highlights) podcast about new research in NLP
- UW NLP [mailing list](https://mailman.cs.washington.edu/mailman/listinfo/uw-nlp) with info about local talks
- [Reading list](https://wammar.github.io/2018sp_uw_cse_599/index.html) from a recent seminar on influential papers in NLP
